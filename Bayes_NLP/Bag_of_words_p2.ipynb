{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "Ref: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "\n",
    "Goal: Using distributed word vectors created by the Word2Vec algorithm. Word2Vec (Google, 2013) is a neural network implementation that learns distributed representations for words. It does not need labels.\n",
    "\n",
    "## Using word2vec in Python\n",
    "we will use **gensim** package. We also need install **cython** otherwise it will take days to run instead of mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Distributed Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read data from files\n",
    "train = pd.read_csv('labeledTrainData.tsv',header=0,delimiter = '\\t',quoting=3)\n",
    "test = pd.read_csv('testData.tsv',header=0, delimiter='\\t',quoting=3)\n",
    "unlabeled_train=pd.read_csv('unlabeledTrainData.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "# verify the number of reviews that were read (100,000 in total)\n",
    "print 'Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n' %(train['review'].size, test['review'].size, unlabeled_train['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    \"\"\"Function to convert a document to a sequence of words, optionally removing stop words. Returns a list of words\n",
    "    \"\"\"\n",
    "    #1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",' ',review_text)\n",
    "    #3. Covert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words =[w for w in words if not w in stops]\n",
    "    #5. return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec expects single sentences, each one as a list of words. It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use NLTK's punkt tokenizer for sentence splitting. In order to use this, you will need to install NLTK and use nltk.download() to download the relevant training file for punkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 1867, in run\n",
      "    for msg in self.data_server.incr_download(self.items):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 529, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 572, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 543, in incr_download\n",
      "    for msg in self.incr_download(info.children, download_dir, force):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 529, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 572, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 549, in incr_download\n",
      "    for msg in self._download_package(info, download_dir, force):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 638, in _download_package\n",
      "    for msg in _unzip_iter(filepath, zipdir, verbose=False):\n",
      "  File \"/Users/yuchengtsai/anaconda/lib/python2.7/site-packages/nltk/downloader.py\", line 2039, in _unzip_iter\n",
      "    outfile.write(contents)\n",
      "IOError: [Errno 28] No space left on device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download()\n",
    "# load the punkt tonkenizer\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review_info parased sentences\n",
    "def review_to_sentence(review, tokenozer, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Function to split a review into parsed senetences. Returns a list of sentences, where each sentence is a list of\n",
    "    words\n",
    "    \"\"\"\n",
    "    #1. use the nltk tokenizer to split the paragraph into sentences\n",
    "    raw_sentences =tokenizer.tokenize(review.strip())\n",
    "    #2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        #If a sentence is empty, skip it\n",
    "        if len(raw_sentence)>0:\n",
    "            #Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence,remove_stopwords))\n",
    "    #return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "    return sentences\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
