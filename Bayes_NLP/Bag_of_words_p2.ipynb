{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "Ref: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "\n",
    "Goal: Using distributed word vectors created by the Word2Vec algorithm. Word2Vec (Google, 2013) is a neural network implementation that learns distributed representations for words. It does not need labels.\n",
    "Using word2vec in Python\n",
    "\n",
    "We will use gensim package. We also need to install cython otherwise it will take days to run instead of minutes.\n",
    "## Preparing to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled traini reviews, 25000 labeled test reviews,and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "start = time.time()\n",
    "# Read data from files\n",
    "train = pd.read_csv('labeledTrainData.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "test = pd.read_csv('testData.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "unlabeled_train=pd.read_csv('unlabeledTrainData.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "# verify the number of the review that were read (100,000 in total)\n",
    "print (\"Read %d labeled traini reviews, %d labeled test reviews,\" \\\n",
    "\"and %d unlabeled reviews\\n\" %(train['review'].size, test['review'].size, unlabeled_train['review'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words. Returns a list of words.\n",
    "    # 1. Remove HTML\n",
    "    review_text=BeautifulSoup(review,\"html\").get_text()\n",
    "    # 2. Remove non-letters, keep numbers\n",
    "    review_text=re.sub(\"[^a-zA-Z0-9]\",\" \",review_text)\n",
    "    # 3. Convert words in lower case and split them\n",
    "    words=review_text.lower().split()\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops=set(stopwords.words(\"english\"))\n",
    "        words=[w for w in words if not w in stops]\n",
    "    # 5. return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences. \n",
    "    # Returns a list of sentences, where each sentence is a list of words.\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences=tokenizer.tokenize(review.strip())\n",
    "    # 2. Loop over each sentence\n",
    "    sentences=[]\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence)>0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence,remove_stopwords))\n",
    "        # Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:198: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.happierabroad.com\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.archive.org/details/LovefromaStranger\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.loosechangeguide.com/LooseChangeGuide.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.msnbc.msn.com/id/4972055/site/newsweek/\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:198: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.youtube.com/watch?v=a0KSqelmgN8\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/amywu/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://jake-weird.blogspot.com/2007/08/beneath.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences+=review_to_sentences(review, tokenizer)\n",
    "    \n",
    "print (\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences+=review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total- should be around 850,000+\n",
    "print (len(sentences))\n",
    "print (sentences[0])\n",
    "print (sentences[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Saving Your Model\n",
    "- **Architecture**: Skip-gram (default) or continuous bags of words. Skip-gram slightly slower but produced better results\n",
    "- **Training algorithm**: Hierarchical softmax (default) or negative sampling. Default works better\n",
    "- **Downsamping of frequent words**: Google documentation recommends between .00001 and .001. Values closer .001 seemed to improve the accuracy of the final model\n",
    "- **Word vector dimensionality**: Reasonable values can be in the tens to hundreds. We used 300.\n",
    "- **Context/window size**: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax\n",
    "- **Worker threads**: Number of parallel processes to run\n",
    "- **Minimum word count**: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n",
      "342.69152307510376 sec\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec create nice output message\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s: %(message)s',level=logging.INFO)\n",
    "# Set values for various parameters:\n",
    "num_features =300 # word vector dimensionality\n",
    "min_word_count =40 # minimum word count\n",
    "num_workers =4 # Number of threads to run in parallel\n",
    "context = 10 # Context window size\n",
    "downsampling = 1e-3 # Downsample setting for frequent words\n",
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print('Training Model...')\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers,size=num_features,min_count=min_word_count,window=context,\\\n",
    "                          sample=downsampling)\n",
    "#If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "#It can be helpful to create a meaningful model name and save the model for latter use. You can load it later using Word2Vec.load()\n",
    "model_name='300features_40minwords_10context'\n",
    "model.save(model_name)\n",
    "end = time.time()\n",
    "print('{} sec'.format((end-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Model Results\n",
    "The 'doesnt_match' function will try to deduce which word in a set is most dissimilar from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('man woman child kitchen'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('france england germany berlin'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('paris berlin london austria'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6193481683731079),\n",
       " ('lady', 0.5889842510223389),\n",
       " ('lad', 0.5411475896835327),\n",
       " ('men', 0.5185449123382568),\n",
       " ('millionaire', 0.5149509310722351),\n",
       " ('guy', 0.5141556262969971),\n",
       " ('monk', 0.5101891756057739),\n",
       " ('farmer', 0.5047257542610168),\n",
       " ('businessman', 0.5012615919113159),\n",
       " ('sailor', 0.5010964274406433)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function \"most_similar\" can get insight into the model's word clusters:\n",
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6624864339828491),\n",
       " ('bride', 0.6191389560699463),\n",
       " ('mistress', 0.6012598276138306),\n",
       " ('victoria', 0.5922946929931641),\n",
       " ('kristel', 0.5868010520935059),\n",
       " ('showgirl', 0.5861167311668396),\n",
       " ('stepmother', 0.582958459854126),\n",
       " ('latifah', 0.5828735828399658),\n",
       " ('goddess', 0.5825923681259155),\n",
       " ('belle', 0.5806471705436707)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7759661674499512),\n",
       " ('atrocious', 0.758520245552063),\n",
       " ('horrible', 0.7496703863143921),\n",
       " ('abysmal', 0.7101701498031616),\n",
       " ('horrendous', 0.7023831605911255),\n",
       " ('dreadful', 0.6928654313087463),\n",
       " ('appalling', 0.6835609078407288),\n",
       " ('horrid', 0.6614154577255249),\n",
       " ('lousy', 0.6244205236434937),\n",
       " ('amateurish', 0.615602970123291)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('awful')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
